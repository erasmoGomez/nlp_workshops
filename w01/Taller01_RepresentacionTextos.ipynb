{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Taller 01: Métodos de Representación"],"metadata":{"id":"rG3kFOC7pUWm"}},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"fYDe7P361oYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wl0aw80537kY"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","source":["El dataset fue obtenido de Kaggle: https://www.kaggle.com/datasets/exactful/wikipedia-movies/data"],"metadata":{"id":"Di63GSgHTkwG"}},{"cell_type":"code","source":["df = pd.read_csv('1990s-movies.csv')\n","df.shape"],"metadata":{"id":"RiUwhzPJ3-0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['plot'].head()"],"metadata":{"id":"sa2yxzzL6mZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocesamiento\n","Antes de poder trabajar con el texto, debemos preprocesarlo, de forma que aseguremos que el input que ingrese a nuestros análisis sea el correcto."],"metadata":{"id":"3XEbfB8QJtaY"}},{"cell_type":"markdown","source":["### 1. Pasar todo a minúsculas"],"metadata":{"id":"zfGRvlwCJ7Li"}},{"cell_type":"code","source":["df['plot'] = df['plot'].str.lower()\n","df['plot'].head()"],"metadata":{"id":"tPtKccR18M2x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Eliminar los espacios en blanco en exceso"],"metadata":{"id":"maqngFjaKMiD"}},{"cell_type":"code","source":["\" \".join(\"hola      mi    nombre  es erasmo\".split())"],"metadata":{"id":"BnJdwnafsFSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['plot'] = df['plot'].apply(lambda x: \" \".join(x.split()))\n","df['plot'].head()"],"metadata":{"id":"IpibABNq8aZ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Tokenizamos las palabras\n","Nota: En algunos casos puede ser necesario tokenizar primero por oraciones y luego por palabras."],"metadata":{"id":"z3X90cJNKc7C"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","df['plot'] = df['plot'].apply(word_tokenize)\n","df['plot'].head()"],"metadata":{"id":"W5-qHZPX-HTR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Remover los stopwords\n","Los stopwords son palabras que se usan con frecuencia, pero no aportan al texto en muchos casos."],"metadata":{"id":"R5BNIvtgKxfL"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","\n","en_stopwords = stopwords.words('english')\n","df['plot'] = df['plot'].apply(lambda x: [y for y in x if y not in en_stopwords])\n","df['plot'].head()"],"metadata":{"id":"nblFPsFUA_TH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Remover signos de puntuación\n","Se eliminan los signos de puntuación, ya que en la mayoría de casos no tienen aporte al significado."],"metadata":{"id":"yqIjbQtALD7N"}},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer\n","\n","tokenizer = RegexpTokenizer(r\"\\w+\")\n","df['plot'] = df['plot'].apply(lambda x: tokenizer.tokenize(' '.join(x)))\n","df['plot'].head()"],"metadata":{"id":"K0_CBaZCCCYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7. Guardamos la forma no tokenizada\n","Para algunos algoritmos, requeriremos la forma no tokenizadas; mientras que para otros sí. Por tanto, los guardamos en columnas separadas."],"metadata":{"id":"pybHiefhMSj2"}},{"cell_type":"code","source":["df['plot2']= df['plot'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"Pn_D2zIfEyHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[['plot','plot2']].head()"],"metadata":{"id":"K-ZQ1i-OGezF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Representación Bag of Words\n","Se puede obtener la representación de Bag of Words fácilmente empleando el CountVectorizer de sklearn. Cabe destacar que esta función tiene algunos argumentos de preproceamiento, por lo que algunos pasos que se ejecutaron previamente estarían duplicados. Revise la documentación en: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"],"metadata":{"id":"kBBgN2IHM5RY"}},{"cell_type":"code","source":["# Se genera un dataset de 2 documentos, solo para visualizar\n","df_disperso = df[:10].copy(deep=True)\n","df_disperso"],"metadata":{"id":"ZeJPev-fX6am"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_disperso['plot2']"],"metadata":{"id":"ar9wvxJXTmJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","#BAG OF WORDS\n","vectorizer = CountVectorizer() #Se crea primero\n","vectorizer.fit(df_disperso['plot2'])"],"metadata":{"id":"NyHQ7CMKGgP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtenemos el vocabulario para saber qué índice corresponde a cada palabra\n","vocabulary = vectorizer.vocabulary_\n","vocabulary = {k: v for k, v in sorted(vocabulary.items(), key=lambda item: item[1])}\n","print(\"Vocabulary: \", vocabulary)"],"metadata":{"id":"zOJapUbcT05o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(vocabulary)"],"metadata":{"id":"4RWIgQk2uNMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_disperso['plot2'][0]"],"metadata":{"id":"RUwRr0W3uF9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_disperso.iloc[0][['plot2']]"],"metadata":{"id":"L7GZR1SbUfeb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora veremos un caso y lo transformaremos a su forma de Bag of Words"],"metadata":{"id":"zSnptFI8QU4Q"}},{"cell_type":"code","source":["# El primer 1 del vector corresponde a la posicion 9. La palabra en esa posición es addiction.\n","vector = vectorizer.transform(df_disperso.iloc[0][['plot2']])\n","print(\"Encoded Document is:\")\n","print(vector.toarray())"],"metadata":{"id":"X0K783kQHIBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(vector.toarray()[0])"],"metadata":{"id":"R8Tq4Nw7UlP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(vector.toarray()[0])"],"metadata":{"id":"faR5nCfCuSw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Corroboramos si la palabra addiction se encuentra en el texto\n","[x for x in df_disperso.iloc[0]['plot2'].split(' ') if x=='addiction']"],"metadata":{"id":"WcxxglNOHWCM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vector.toarray()[0]"],"metadata":{"id":"GnXvBVh0U8-P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"099b5f6d"},"source":["# Introducción a Word2Vec\n","\n","**Word2Vec** es una técnica computacional que convierte palabras en vectores de números, representando el significado y relaciones entre ellas. Si dos palabras aparecen en contextos similares, sus vectores también serán similares.\n","\n","### Ejemplo práctico con gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d93d7e17"},"outputs":[],"source":["# Si no tienes gensim instalado, ejecuta esto:\n","!pip install gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14b2e32a"},"outputs":[],"source":["# Corpus sencillo tokenizado\n","corpus = [\n","    [\"el\", \"gato\", \"se\", \"sentó\", \"en\", \"la\", \"alfombra\"],\n","    [\"el\", \"perro\", \"se\", \"echó\", \"en\", \"la\", \"alfombra\"],\n","    [\"la\", \"reina\", \"y\", \"el\", \"rey\", \"viven\", \"en\", \"el\", \"castillo\"],\n","    [\"el\", \"gato\", \"y\", \"el\", \"perro\", \"son\", \"amigos\"],\n","    [\"la\", \"princesa\", \"es\", \"hija\", \"de\", \"la\", \"reina\"],\n","    [\"la\", \"reina\", \"es\", \"una\",\"mujer\"],\n","    [\"el\", \"rey\", \"es\", \"un\",\"hombre\"],\n","    [\"el\", \"gato\", \"come\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"gato\", \"duerme\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"perro\", \"come\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"perro\", \"duerme\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"gato\", \"come\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"rey\", \"come\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"rey\", \"duerme\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"princesa\", \"come\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"gato\", \"come\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"gato\", \"duerme\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"perro\", \"come\", \"en\",\"el\", \"castillo\"],\n","    [\"el\", \"perro\", \"duerme\", \"en\",\"el\", \"castillo\"]\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ef0b65b"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","modelo = Word2Vec(corpus, vector_size=50, window=2, min_count=1, sg=1) #parametros iniciales"]},{"cell_type":"markdown","metadata":{"id":"6cd42485"},"source":["### Palabras similares a 'reina'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"001cc13a"},"outputs":[],"source":["similares = modelo.wv.most_similar(\"gato\")\n","print(\"Palabras similares a 'gato':\")\n","similares"]},{"cell_type":"code","source":["for palabra, similitud in similares:\n","    print(palabra, \"->\", similitud)"],"metadata":{"id":"SjhVmevCYUmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2f25078a"},"source":["### Analogías: rey - hombre + mujer ≈ reina"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7939b3d9"},"outputs":[],"source":["resultado = modelo.wv.most_similar(positive=[\"rey\", \"mujer\"], negative=[\"hombre\"])\n","print(\"Resultado de la analogía rey - hombre + mujer:\", resultado[0][0])"]},{"cell_type":"markdown","metadata":{"id":"b9df8cd9"},"source":["### Vector numérico de una palabra ('gato')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62fa2db6"},"outputs":[],"source":["print(\"Vector de 'gato':\")\n","print(modelo.wv[\"gato\"])"]},{"cell_type":"code","source":["len(modelo.wv[\"gato\"])"],"metadata":{"id":"avu-GV_3Zhva"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fa66fd5d"},"source":["### Visualización de palabras en el espacio semántico\n","Ahora vamos a graficar cómo Word2Vec ubicó cada palabra en un espacio de 2 dimensiones usando PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17debd50"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","\n","palabras = list(modelo.wv.index_to_key)\n","vectores = np.array([modelo.wv[palabra] for palabra in palabras])\n","\n","pca = PCA(n_components=2)\n","vectores_2d = pca.fit_transform(vectores)\n","\n","plt.figure(figsize=(10, 7))\n","plt.scatter(vectores_2d[:, 0], vectores_2d[:, 1])\n","for i, palabra in enumerate(palabras):\n","    plt.annotate(palabra, (vectores_2d[i, 0], vectores_2d[i, 1]), fontsize=12)\n","plt.title(\"Visualización de palabras en el espacio de Word2Vec (PCA)\")\n","plt.xlabel(\"Componente 1\")\n","plt.ylabel(\"Componente 2\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"a38fb970"},"source":["Este gráfico muestra cómo las palabras se agrupan según su significado/contexto en el espacio de Word2Vec."]}]}